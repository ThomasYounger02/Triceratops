{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe04231d",
   "metadata": {},
   "source": [
    "# 什么是爬虫？\n",
    "爬虫，从本质上来说，就是利用程序在网上拿到对我们有价值的数据。\n",
    "\n",
    "爬虫能做很多事，能做商业分析，也能做生活助手，比如：分析北京近两年二手房成交均价是多少？深圳的Python工程师平均薪资是多少？北京哪家餐厅粤菜最好吃？等等。\n",
    "\n",
    "搜索引擎——百度和谷歌，它们的核心技术之一也是爬虫，而且是**超级爬虫**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83acd2cd",
   "metadata": {},
   "source": [
    "# 浏览器的工作原理\n",
    "浏览器和服务器之间，先请求，后响应。\n",
    "\n",
    "当服务器把数据响应给浏览器之后，浏览器并不会直接把数据丢给你。因为这些数据是用计算机的语言写的，浏览器还要把这些数据翻译成你能看得懂的样子，这是浏览器做的另一项工作**解析数据**。\n",
    "<img src=\"pics/browser_principle.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf74e61",
   "metadata": {},
   "source": [
    "# 爬虫的工作原理\n",
    "<img src=\"pics/spider_principle.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45cd79",
   "metadata": {},
   "source": [
    "## 爬虫的4个步骤：\n",
    "\n",
    "- **获取数据**:爬虫程序会根据我们提供的网址，向服务器发起请求，然后返回数据。\n",
    "- **解析数据**:爬虫程序会把服务器返回的数据解析成我们能读懂的格式。\n",
    "- **提取数据**:爬虫程序再从中提取出我们需要的数据。\n",
    "- **储存数据**:爬虫程序把这些有用的数据保存起来，便于你日后的使用和分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675688e",
   "metadata": {},
   "source": [
    "## 爬虫学习的重点：\n",
    "\n",
    "- 爬虫的工作原理；\n",
    "- HTML基础知识，达到读懂和修改HTML文档的水平；\n",
    "- 解析数据和提取数据，不同的发起请求的方式；\n",
    "- 存储数据；\n",
    "- 实战项目训练；\n",
    "- 学会cookies，让浏览器记住你；\n",
    "- 学习控制浏览器，来应对爬虫中一些更复杂的情况；\n",
    "- 爬虫变得更自动化；\n",
    "- 升级爬虫，爬虫为你消灭重复劳动，高效获取信息，创造出更多价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b13c03",
   "metadata": {},
   "source": [
    "# requests库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74545ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # 引入requests库\n",
    "\n",
    "url = 'https://www.baidu.com'\n",
    "res = requests.get(url)   # requests.get是在调用requests库中的get()方法，它向服务器发送了一个请求\n",
    "# 括号里的参数是你需要的数据所在的网址，然后服务器对请求作出了响应。\n",
    "# 我们把这个响应返回的结果赋值在变量res上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef763a",
   "metadata": {},
   "source": [
    "## response对象的常用属性\n",
    "\n",
    "<img src=\"pics/requests.png\" width=\"50%\">\n",
    "Python是一门面向对象编程的语言，而在爬虫中，理解数据是什么对象是非常、特别、以及极其重要的一件事。因为只有知道了数据是什么对象，我们才知道对象有什么属性和方法可供我们操作。\n",
    "res是一个对象，属于requests.models.Response类。既然已经知道res是一个Response对象了，我们也就可以去了解它的相应属性和方法了。\n",
    "<img src=\"pics/response_attr.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae0e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "#打印变量res的数据类型\n",
    "print(type(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b211c",
   "metadata": {},
   "source": [
    "## status_code\n",
    "<img src=\"pics/status_code.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a39ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(res.status_code)\n",
    "#打印变量res的响应状态码，以检查请求是否成功，显示200即代表成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ed27f",
   "metadata": {},
   "source": [
    "## response.content\n",
    "\n",
    "把Response对象的内容以二进制数据的形式返回，适用于图片、音频、视频的下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ecd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://res.pandateacher.com/2018-12-18-10-43-07.png'\n",
    "res = requests.get(url)                # 发出请求，并把返回的结果放在变量res中\n",
    "pic=res.content                        # 把Reponse对象的内容以二进制数据的形式返回\n",
    "photo = open('result/ppt.jpg','wb')    # 新建了一个文件ppt.jpg，这里的文件没加路径，它会被保存在程序运行的当前目录下。\n",
    "#图片内容需要以二进制wb读写。\n",
    "\n",
    "photo.write(pic)                       # 获取pic的二进制内容\n",
    "photo.close()                          # 关闭文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb882d9",
   "metadata": {},
   "source": [
    "## response.text\n",
    "\n",
    "text属性可以把Response对象的内容以字符串的形式返回，适用于文字、网页源代码的下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea43056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 三国演义\r\n",
      "\r\n",
      "作者：罗贯中\r\n",
      "\r\n",
      "### 第一回 宴桃园豪杰三结义 斩黄巾英雄首立功\r\n",
      "\r\n",
      "> 滚滚长江东逝水，浪花淘尽英雄。\r\n",
      "> 是非成败转头空。\r\n",
      "> 青山依旧在，几度夕阳红。　　\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md'\n",
    "res = requests.get(url)         # 下载《三国演义》第一回，我们得到一个对象，它被命名为res\n",
    "novel=res.text                  # 把Response对象的内容以字符串的形式返回\n",
    "print(novel[:99])               # 查看文本信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488876c",
   "metadata": {},
   "source": [
    "## response.encoding\n",
    "\n",
    "它能帮我们定义Response对象的编码。\n",
    "\n",
    "目标数据本身是什么编码是未知的。用requests.get()发送请求后，我们会取得一个Response对象，其中，requests库会对数据的编码类型做出自己的判断。但是这个判断有可能准确，也可能不准确。\n",
    "\n",
    "如果它判断准确的话，我们打印出来的response.text的内容就是正常的、没有乱码的，那就用不到res.encoding；\n",
    "\n",
    "如果判断不准确，就会出现一堆乱码，那我们就可以去查看目标数据的编码，然后再用res.encoding把编码定义成和目标数据一致的类型即可。\n",
    "\n",
    "> 遇上文本的**乱码问题**，才考虑用res.encoding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "604c16f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 涓夊浗婕斾箟\r\n",
      "\r\n",
      "浣滆�咃細缃楄疮涓�\r\n",
      "\r\n",
      "### 绗�涓�鍥� 瀹存�冨洯璞�鏉颁笁缁撲箟 鏂╅粍宸捐嫳闆勯�栫珛鍔�\r\n",
      "\r\n",
      "> 婊氭粴闀挎睙涓滈�濇按锛屾氮鑺辨窐灏借嫳闆勩��\r\n",
      "> \n"
     ]
    }
   ],
   "source": [
    "url = 'https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md'\n",
    "res = requests.get(url)       # 下载《三国演义》第一回，我们得到一个对象，它被命名为res\n",
    "res.encoding='gbk'            # 定义Response对象的编码为gbk\n",
    "novel=res.text                # 把Response对象的内容以字符串的形式返回\n",
    "print(novel[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123e298",
   "metadata": {},
   "source": [
    "# 爬虫伦理\n",
    "服务器其实就是一个超级电脑，拥有这个服务器的公司，对爬虫其实也有明确的态度。\n",
    "\n",
    "通常情况下，服务器不太会在意小爬虫，但是，服务器会拒绝频率很高的大型爬虫和恶意爬虫，因为这会给服务器带来极大的压力或伤害。\n",
    "\n",
    "服务器在通常情况下，对搜索引擎是欢迎的态度。当然，这是有条件的，而这些条件会写在**Robots协议**。\n",
    "\n",
    "Robots协议是互联网爬虫的一项公认的道德规范，它的全称是**网络爬虫排除标准（Robots exclusion protocol）**，这个协议用来告诉爬虫，哪些页面是可以抓取的，哪些不可以。\n",
    "\n",
    "在网站的域名后加上`/robots.txt`就可以查看网站的robots协议。\n",
    "\n",
    "淘宝的robots协议 （ [http://www.taobao.com/robots.txt）](http://www.taobao.com/robots.txt%EF%BC%89)。\n",
    "\n",
    "爬虫就像是核技术，人们可以利用它去做有用的事，也能利用它去搞破坏。\n",
    "\n",
    "恶意消耗别人的服务器资源，是一件不道德的事，恶意爬取一些不被允许的数据，还可能会引起严重的法律后果。\n",
    "\n",
    "当你在爬取网站数据的时候，别忘了先看看网站的<font color=\"#dd0000\">**Robots协议是否允许**</font><br /> 你去爬取。\n",
    "\n",
    "限制好<font color=\"#dd0000\">**爬虫的速度**</font><br /> ，对提供数据的服务器心存感谢，避免给它造成太大压力，维持良好的互联网秩序，也是我们该做的事。\n",
    "\n",
    "爬虫的本质是利用程序帮我们获取有价值的信息，爬虫程序可以消灭重复劳动，并且创造价值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
