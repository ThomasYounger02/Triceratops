{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e315510",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "能不能有一个现成的爬虫模板，让我们拿来就能套用，就像PPT模板一样。我们不需要管爬虫的全部流程，只要负责填充好爬虫的核心逻辑代码就好。要是有的话，我们编写代码一定会很方便省事。\n",
    "\n",
    "在Python中还真的存在这样的爬虫模板，只不过它的名字是叫框架。\n",
    "\n",
    "安装\n",
    "\n",
    "`pip install scrapy`\n",
    "\n",
    "## Scrapy是什么？\n",
    "\n",
    "以前我们写爬虫，要导入和操作不同的模块，比如requests模块、gevent库、csv模块等。而在Scrapy里，你不需要这么做，因为很多爬虫需要涉及的功能，比如麻烦的异步，在Scrapy框架都自动实现了。\n",
    "\n",
    "之前编写爬虫的方式，相当于在一个个地在拼零件，拼成一辆能跑的车。而Scrapy框架则是已经造好的、现成的车，我们只要踩下它的油门，它就能跑起来。这样便节省了我们开发项目的时间。\n",
    "\n",
    "## Scrapy的结构\n",
    "<img src=\"pics/scrapy_结构1.png\" width=\"40%\">\n",
    "把整个Scrapy框架看成是一家爬虫公司。最中心位置的Scrapy Engine(引擎）就是这家爬虫公司的大boss，负责统筹公司的4大部门，每个部门都只听从它的命令，并只向它汇报工作。\n",
    "\n",
    "**Scheduler(调度器)**部门主要负责处理引擎发送过来的requests对象（即网页请求的相关信息集合，包括params，data，cookies，request headers…等），会把请求的url以有序的方式排列成队，并等待引擎来提取（功能上类似于gevent库的queue模块）。\n",
    "\n",
    "**Downloader（下载器）**部门则是负责处理引擎发送过来的requests，进行网页爬取，并将返回的response（爬取到的内容）交给引擎。它对应的是爬虫流程【获取数据】这一步。\n",
    "\n",
    "**Spiders(爬虫)**部门是公司的核心业务部门，主要任务是创建requests对象和接受引擎发送过来的response（Downloader部门爬取到的内容），从中解析并提取出有用的数据。它对应的是爬虫流程【解析数据】和【提取数据】这两步。\n",
    "\n",
    "**Item Pipeline（数据管道）**部门则是公司的数据部门，只负责存储和处理Spiders部门提取到的有用数据。这个对应的是爬虫流程【存储数据】这一步。\n",
    "\n",
    "**Downloader Middlewares（下载中间件）**的工作相当于下载器部门的秘书，比如会提前对引擎大boss发送的诸多requests做出处理。\n",
    "\n",
    "**Spider Middlewares（爬虫中间件）**的工作则相当于爬虫部门的秘书，比如会提前接收并处理引擎大boss发送来的response，过滤掉一些重复无用的东西。\n",
    "<img src=\"pics/scrapy_结构2.png\" width=\"40%\">\n",
    "\n",
    "## Scrapy的工作原理\n",
    "<img src=\"pics/scrapy_原理.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc863d7",
   "metadata": {},
   "source": [
    "在Scrapy里，整个爬虫程序的流程都不需要我们去操心，且Scrapy中的程序全部都是异步模式，所有的请求或返回的响应都由引擎自动分配去处理。\n",
    "# Scrapy的用法\n",
    "\n",
    "## 明确目标与分析过程\n",
    "\n",
    "### 明确目标\n",
    "\n",
    "豆瓣Top250图书的链接：https://book.douban.com/top250\n",
    "\n",
    "### 代码实现——创建项目\n",
    "\n",
    "在本地电脑打开终端（windows：Win+R，输入cmd；mac：command+空格，搜索“终端”），然后跳转到你想要保存项目的目录下。\n",
    "\n",
    "`scrapy startproject douban`\n",
    "\n",
    "douban就是Scrapy项目的名字。按下enter键，一个Scrapy项目就创建成功了。\n",
    "<img src=\"pics/scrapy_文件组织.png\" width=\"40%\">\n",
    "\n",
    "Scrapy项目里每个文件都有特定的功能，[比如settings.py](http://xn--settings-jo1ot97l.py/) 是scrapy里的各种设置。items.py是用来定义数据的，pipelines.py是用来处理数据的，它们对应的就是Scrapy的结构中的Item Pipeline（数据管道）。\n",
    "\n",
    "## 代码实现——编辑爬虫\n",
    "\n",
    "spiders是放置爬虫的目录。我们可以在spiders这个文件夹里创建爬虫文件。我们来把这个文件，命名为top250。后面的大部分代码都需要在这个top250.py文件里编写。\n",
    "\n",
    "在Scrapy中，每个爬虫的代码结构基本都如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c35e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanSpider(scrapy.Spider):    # 定义一个爬虫类DoubanSpider。就像我刚刚讲过的那样，DoubanSpider类继承自scrapy.Spider类。\n",
    "    name = 'douban'    #n ame是定义爬虫的名字，这个名字是爬虫的唯一标识。name = 'douban'意思是定义爬虫的名字为douban。等会我们启动爬虫的时候，要用到这个名字。\n",
    "    allowed_domains = ['book.douban.com']    # allowed_domains是定义允许爬虫爬取的网址域名（不需要加https://）。如果网址的域名不在这个列表里，就会被过滤掉。\n",
    "    start_urls = ['https://book.douban.com/top250?start=0']    # start_urls是定义起始网址，就是爬虫从哪个网址开始抓取。在此，allowed_domains的设定对start_urls里的网址不会有影响。\n",
    "    \n",
    "    def parse(self, response):    # parse是Scrapy里默认处理response的一个方法，中文是解析。\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc83be",
   "metadata": {},
   "source": [
    "allowed_domains就限制了，我们这种关联爬取的URL，一定在book.douban.com这个域名之下，不会跳转到某个奇怪的广告页面。\n",
    "\n",
    "改进："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import bs4\n",
    "from ..items import DoubanItem\n",
    "\n",
    "class DoubanSpider(scrapy.Spider):\n",
    "#定义一个爬虫类DoubanSpider。\n",
    "    name = 'douban'\n",
    "    #定义爬虫的名字为douban。\n",
    "    allowed_domains = ['book.douban.com']\n",
    "    #定义爬虫爬取网址的域名。\n",
    "    start_urls = []\n",
    "    #定义起始网址。\n",
    "    for x in range(3):\n",
    "        url = 'https://book.douban.com/top250?start=' + str(x * 25)\n",
    "        start_urls.append(url)\n",
    "        #把豆瓣Top250图书的前3页网址添加进start_urls。\n",
    "\n",
    "    def parse(self, response):\n",
    "    #parse是默认处理response的方法。\n",
    "        bs = bs4.BeautifulSoup(response.text,'html.parser')\n",
    "        #用BeautifulSoup解析response。\n",
    "        datas = bs.find_all('tr',class_=\"item\")\n",
    "        #用find_all提取<tr class=\"item\">元素，这个元素里含有书籍信息。\n",
    "        for data in  datas:\n",
    "        #遍历data。\n",
    "            title = data.find_all('a')[1]['title']\n",
    "            #提取出书名。\n",
    "            publish = data.find('p',class_='pl').text\n",
    "            #提取出出版信息。\n",
    "            score = data.find('span',class_='rating_nums').text\n",
    "            #提取出评分。\n",
    "            print([title,publish,score])\n",
    "            #打印上述信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21743e",
   "metadata": {},
   "source": [
    "## 代码实现——定义数据\n",
    "\n",
    "每一次，当数据完成记录，它会离开spiders，来到Scrapy Engine（引擎），引擎将它送入Item Pipeline（数据管道）处理。\n",
    "\n",
    "定义这个类的py文件，正是items.py。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8114b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "#导入scrapy\n",
    "class DoubanItem(scrapy.Item):\n",
    "#定义一个类DoubanItem，它继承自scrapy.Item\n",
    "    title = scrapy.Field()\n",
    "    #定义书名的数据属性\n",
    "    publish = scrapy.Field()\n",
    "    #定义出版信息的数据属性\n",
    "    score = scrapy.Field()\n",
    "    #定义评分的数据属性\n",
    "\n",
    "book = DoubanItem()\n",
    "# 实例化一个DoubanItem对象\n",
    "book['title'] = '海边的卡夫卡'\n",
    "book['publish'] = '[日] 村上春树 / 林少华 / 上海译文出版社 / 2003'\n",
    "book['score'] = '8.1'\n",
    "print(book)\n",
    "print(type(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d61214",
   "metadata": {},
   "source": [
    "yield语句有点类似return，不过它和return不同的点在于，它不会结束函数，且能多次返回信息。\n",
    "\n",
    "## 代码实操——设置\n",
    "\n",
    "修改请求头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee94a67",
   "metadata": {},
   "source": [
    "把USER _AGENT的注释取消（删除#），然后替换掉user-agent的内容，就是修改了请求头。\n",
    "\n",
    "因为Scrapy是遵守robots协议的，如果是robots协议禁止爬取的内容，Scrapy也会默认不去爬取，所以我们还得修改Scrapy中的默认设置。\n",
    "\n",
    "把ROBOTSTXT_OBEY=True改成ROBOTSTXT_OBEY=False，就是把遵守robots协议换成无需遵从robots协议，这样Scrapy就能不受限制地运行。\n",
    "\n",
    "## 代码实操——运行\n",
    "\n",
    "运行Scrapy有两种方法，一种是在本地电脑的终端跳转到scrapy项目的文件夹（跳转方法：cd+文件夹的路径名），然后输入命令行：scrapy crawl douban（douban 就是我们爬虫的名字）。\n",
    "\n",
    "另一种运行方式需要我们在最外层的大文件夹里新建一个main.py文件（与scrapy.cfg同级）。\n",
    "\n",
    "只需要在这个main.py文件里，输入以下代码，点击运行，Scrapy的程序就会启动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import cmdline    #在Scrapy中有一个可以控制终端命令的模块cmdline。导入了这个模块，我们就能操控终端。\n",
    "#导入cmdline模块,可以实现控制终端命令行。\n",
    "cmdline.execute(['scrapy','crawl','douban'])    #在cmdline模块中，有一个execute方法能执行终端的命令行，不过这个方法需要传入列表的参数。我们想输入运行Scrapy的代码scrapy crawl douban，就需要写成['scrapy','crawl','douban']这样。\n",
    "#用execute（）方法，输入运行scrapy的命令。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
