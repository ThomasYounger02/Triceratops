{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaaf3436",
   "metadata": {},
   "source": [
    "# 协程是什么？\n",
    "\n",
    "问题：因为程序是一行一行依次执行的缘故，要等待很久，我们才能拿到想要的数据。\n",
    "\n",
    "在一个任务未完成时，就可以执行其他多个任务，彼此不受影响（在看第一部下载好的电影时，其他电影继续保持下载状态，彼此之间不受影响），叫**异步**。\n",
    "\n",
    "**同步**就是一个任务结束才能启动下一个（类比你看完一部电影，才能去看下一部电影）。\n",
    "\n",
    "很多时候，由于网络不稳定，加上服务器自身也需要响应的时间，导致爬虫会浪费大量时间在等待上。这也是爬取大量数据时，爬虫的速度会比较慢的原因。\n",
    "\n",
    "单核CPU的计算机在处理多任务时，会出现一个问题：每个任务都要抢占CPU，执行完了一个任务才开启下一个任务。\n",
    "<img src=\"pics/协程_示例.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "为了解决这样的问题，一种非抢占式的异步技术被创造了出来，这种方式叫**多协程**（在此，多是多个的意思）。\n",
    "\n",
    "原理是：一个任务在执行过程中，如果遇到等待，就先去执行其他的任务，当等待结束，再回来继续之前的那个任务。在计算机的世界，这种任务来回切换得非常快速，看上去就像多个任务在被同时执行一样。\n",
    "\n",
    "## 多协程的用法\n",
    "\n",
    "多协程，是一种非抢占式的异步方式。使用多协程的话，就能让多个爬取任务用异步的方式交替执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e109bf5",
   "metadata": {},
   "source": [
    "# gevent库\n",
    "同步的爬虫方式爬取这8个网站："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,time\n",
    "#导入requests和time\n",
    "start = time.time()\n",
    "#记录程序开始时间\n",
    "\n",
    "url_list = ['https://www.baidu.com/',\n",
    "'https://www.sina.com.cn/',\n",
    "'http://www.sohu.com/',\n",
    "'https://www.qq.com/',\n",
    "'https://www.163.com/',\n",
    "'http://www.iqiyi.com/',\n",
    "'https://www.tmall.com/',\n",
    "'http://www.ifeng.com/']\n",
    "#把8个网站封装成列表\n",
    "\n",
    "for url in url_list:\n",
    "#遍历url_list\n",
    "    r = requests.get(url)\n",
    "    #用requests.get()函数爬取网站\n",
    "    print(url,r.status_code)\n",
    "    #打印网址和抓取请求的状态码\n",
    "\n",
    "end = time.time()\n",
    "#记录程序结束时间\n",
    "print(end-start)\n",
    "#end-start是结束时间减去开始时间，就是最终所花时间。\n",
    "#最后，把时间打印出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42735218",
   "metadata": {},
   "source": [
    "多协程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all()\n",
    "import gevent,time,requests\n",
    "#从gevent库里导入了monkey模块，这个模块能将程序转换成可异步的程序。\n",
    "#monkey.patch_all()，它的作用其实就像你的电脑有时会弹出“是否要用补丁修补漏洞或更新”一样。\n",
    "#它能给程序打上补丁，让程序变成是异步模式，而不是同步模式。它也叫“猴子补丁”。\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "url_list = ['https://www.baidu.com/',\n",
    "'https://www.sina.com.cn/',\n",
    "'http://www.sohu.com/',\n",
    "'https://www.qq.com/',\n",
    "'https://www.163.com/',\n",
    "'http://www.iqiyi.com/',\n",
    "'https://www.tmall.com/',\n",
    "'http://www.ifeng.com/']\n",
    "\n",
    "def crawler(url):\n",
    "    r = requests.get(url)\n",
    "    print(url,time.time()-start,r.status_code)\n",
    "#我们定义了一个crawler函数，只要调用这个函数，\n",
    "#它就会执行【用requests.get()爬取网站】和【打印网址、请求运行时间、状态码】这两个任务。\n",
    "\n",
    "tasks_list = []\n",
    "\n",
    "for url in url_list:\n",
    "    task = gevent.spawn(crawler,url)\n",
    "    tasks_list.append(task)\n",
    "gevent.joinall(tasks_list)\n",
    "#gevent只能处理gevent的任务对象，不能直接调用普通函数，\n",
    "#所以需要借助gevent.spawn()来创建任务对象。\n",
    "#gevent.spawn()的参数需为要调用的函数名及该函数的参数。\n",
    "#比如，gevent.spawn(crawler,url)就是创建一个执行crawler函数的任务，参数为crawler函数名和它自身的参数url。\n",
    "#用append函数把任务添加到tasks_list的任务列表里。\n",
    "#调用gevent库里的joinall方法，能启动执行所有的任务。gevent.joinall(tasks_list)就是执行tasks_list这个任务列表里的所有任务，开始爬取。\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8920acd",
   "metadata": {},
   "source": [
    "## 1000个任务\n",
    "\n",
    "直接创建1000个任务的方式不可取，那我们能不能只创建成5个任务，但每个任务爬取200个网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a138f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all()\n",
    "import gevent,time,requests\n",
    "\n",
    "start = time.time()\n",
    "url_list = ['https://www.baidu.com/',\n",
    "'https://www.sina.com.cn/',\n",
    "'http://www.sohu.com/',\n",
    "'https://www.qq.com/',\n",
    "'https://www.163.com/',\n",
    "'http://www.iqiyi.com/',\n",
    "'https://www.tmall.com/',\n",
    "'http://www.ifeng.com/'\n",
    "……\n",
    "#假设有1000个网址\n",
    "]\n",
    "\n",
    "def crawler(url_list):\n",
    "#定义一个crawler()函数。\n",
    "    for url in url_list:\n",
    "        r = requests.get(url)\n",
    "        print(url,time.time()-start,r.status_code)\n",
    "\n",
    "tasks_list = [ ]\n",
    "#创建空的任务列表。\n",
    "for i in range(5):\n",
    "    task = gevent.spawn(crawler,url_list[i*200:(i+1)*200])\n",
    "    #用gevent.spawn()函数创建5个任务。\n",
    "    tasks_list.append(task)\n",
    "    #往任务列表添加任务。\n",
    "\n",
    "gevent.joinall(tasks_list)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde08a13",
   "metadata": {},
   "source": [
    "这么做也还是会有问题的。就算我们用gevent.spawn()创建了5个分别执行爬取200个网站的任务，这5个任务之间是异步执行的，但是每个任务（爬取200个网站）内部是同步的。\n",
    "\n",
    "# queue模块\n",
    "\n",
    "当我们用多协程来爬虫，需要创建大量任务时，我们可以借助queue模块。\n",
    "\n",
    "queue翻译成中文是队列的意思。我们可以用queue模块来存储任务，让任务都变成一条整齐的队列，就像银行窗口的排号做法。因为queue其实是一种有序的数据结构，可以用来存取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421101f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gevent import monkey\n",
    "#从gevent库里导入monkey模块。\n",
    "monkey.patch_all()\n",
    "#monkey.patch_all()能把程序变成协作式运行，就是可以帮助程序实现异步。\n",
    "import gevent,time,requests\n",
    "#导入gevent、time、requests\n",
    "from gevent.queue import Queue\n",
    "#从gevent库里导入queue模块\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "url_list = ['https://www.baidu.com/',\n",
    "'https://www.sina.com.cn/',\n",
    "'http://www.sohu.com/',\n",
    "'https://www.qq.com/',\n",
    "'https://www.163.com/',\n",
    "'http://www.iqiyi.com/',\n",
    "'https://www.tmall.com/',\n",
    "'http://www.ifeng.com/']\n",
    "\n",
    "work = Queue()\n",
    "#创建队列对象，并赋值给work。\n",
    "for url in url_list:\n",
    "#遍历url_list\n",
    "    work.put_nowait(url)\n",
    "    #用put_nowait()函数可以把网址都放进队列里。\n",
    "    #work.put_nowait(url)这行代码就是把遍历的8个网站，都存储进队列里。\n",
    "\n",
    "def crawler():\n",
    "    while not work.empty():\n",
    "    #当队列不是空的时候，就执行下面的程序。\n",
    "        url = work.get_nowait()\n",
    "        #用get_nowait()函数可以把队列里的网址都取出。\n",
    "        r = requests.get(url)\n",
    "        #用requests.get()函数抓取网址。\n",
    "        print(url,work.qsize(),r.status_code)\n",
    "        #打印网址、队列长度、抓取请求的状态码。\n",
    "\n",
    "tasks_list  = [ ]\n",
    "#创建空的任务列表\n",
    "for x in range(2):\n",
    "#相当于创建了2个爬虫\n",
    "    task = gevent.spawn(crawler)\n",
    "    #用gevent.spawn()函数创建执行crawler()函数的任务。\n",
    "    tasks_list.append(task)\n",
    "    #往任务列表添加任务。\n",
    "gevent.joinall(tasks_list)\n",
    "#用gevent.joinall方法，执行任务列表里的所有任务，就是让爬虫开始爬取网站。\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90a47a",
   "metadata": {},
   "source": [
    "<img src=\"pics/协程_队列.png\" width=\"40%\">\n",
    "创建了两只可以异步爬取的爬虫。它们会从队列里取走网址，执行爬取任务。一旦一个网址被一只爬虫取走，另一只爬虫就取不到了，另一只爬虫就会取走下一个网址。直至所有网址都被取走，队列为空时，爬虫就停止工作。\n",
    "\n",
    "用Queue()能创建queue对象，相当于创建了一个不限任何存储数量的空队列。如果我们往Queue()中传入参数，比如Queue(10)，则表示这个队列只能存储10个任务。\n",
    "<img src=\"pics/协程_queue对象.png\" width=\"40%\">\n",
    "CPU从单核终于进化到了多核，每个核都能够独立运作。计算机开始能够真正意义上同时执行多个任务（术语叫并行执行），而不是在多个任务之间来回切换（术语叫并发执行）。\n",
    "\n",
    "多协程，其实只占用了CPU的一个核运行，没有充分利用到其他核。利用CPU的多个核同时执行任务的技术，我们把它叫做“多进程”。\n",
    "\n",
    "真正大型的爬虫程序不会单单只靠多协程来提升爬取速度的。比如，百度搜索引擎，可以说是超大型的爬虫程序，它除了靠多协程，一定还会靠多进程，甚至是分布式爬虫。\n",
    "\n",
    "多进程爬虫和分布式爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
