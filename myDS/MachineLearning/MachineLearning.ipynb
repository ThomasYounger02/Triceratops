{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd423f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e0b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "befc254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85180fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4705ad1d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c01237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iris = sns.load_dataset(\"iris\")\n",
    "data_iris.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe00be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(data_iris['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113ca5d",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421f4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yijiu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = LogisticRegression()    # 首先定义分类器，然后使分类器与数据拟合\n",
    "classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f78d5a",
   "metadata": {},
   "source": [
    "# 调优\n",
    "随着数据集的越来越庞大和复杂，选择正确的算法和正确的参数，这一流程将非常枯燥。必须使用“自动”而非“手动”的方式，来完成这一流程。\n",
    "\n",
    "- How do you convince yourself or others that your model is good?\n",
    "- How do we find a model that generalizes well?泛化能力\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d23d6b",
   "metadata": {},
   "source": [
    "## 手动调整参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599a9d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# kernel：linear (线性)， poly（多项式）, rbf（高斯核）\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# degree：多项式内核的次数（如果选择了多项式内核）\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# gamma ：γ 参数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# C：C参数\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m SVC(kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "# kernel：linear (线性)， poly（多项式）, rbf（高斯核）\n",
    "# degree：多项式内核的次数（如果选择了多项式内核）\n",
    "# gamma ：γ 参数\n",
    "# C：C参数\n",
    "\n",
    "classifier = SVC(kernel = 'poly', degree = 2, gamma=200, C=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97656a71",
   "metadata": {},
   "source": [
    "# 模型评估\n",
    "- **How well is my modeling doing?**\n",
    "- **How do we improve the model based on these metrics?**\n",
    "- **How to train, test, evaluate and validate models?**\n",
    "\n",
    "> Thou shalt never use your testing data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a847f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55a00c",
   "metadata": {},
   "source": [
    "# 评估指标-分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bae14",
   "metadata": {},
   "source": [
    "## 混淆矩阵Confusion Matrix\n",
    "\n",
    "- **类型1错误（第一类错误或假阳性）：**误诊一个健康人为病人；\n",
    "- **类型2错误（第二类错误或假阴性）：**漏诊一个病人为健康人；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f1549",
   "metadata": {},
   "source": [
    "### 准确率Accuracy\n",
    "准确率不适用的情况：数据是倾斜分布的（信用卡欺诈数据）\n",
    "\n",
    "### 精度Precision\n",
    "$P = \\frac{TP}{TP+FP}$\n",
    "### 召回率Recall\n",
    "$R = \\frac{TP}{TP+FN}$\n",
    "\n",
    "### F1 Score\n",
    "将精度和召回率结合成为一个指标。\n",
    "\n",
    "F1 Score是精度和召回率的调和平均数(Harmonic mean)，更接近精度和召回率中`最小`的值。   \n",
    "$F1_{score} = \\frac{2PR}{P+R}$\n",
    "\n",
    "\n",
    "拓展：   \n",
    "$F\\beta_{score}=(1+\\beta^2)\\frac{PR}{\\beta^2P+R}$\n",
    "\n",
    "beta越小，越靠近精度，beta越大，越靠近召回率。\n",
    "\n",
    "找到合适的beta需要对数据的直觉和大量的实验。\n",
    "\n",
    "### ROC曲线Receiver Operating Characteristic\n",
    "受试者工作特性曲线\n",
    "\n",
    "设计一个指标：完美的划分得分在1，好的划分在0.8， 随机的划分在0.5。\n",
    "\n",
    "真阳性比例：   \n",
    "$TPR = \\frac{TP}{TP+FN}$\n",
    "\n",
    "假阳性比例：\n",
    "$FPR=\\frac{FP}{FP+TN}$\n",
    "\n",
    "\n",
    ">ROC曲线下的面积越接近1，模型的效果越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34406d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator, \n",
    "    X, \n",
    "    y, \n",
    "    cv=None, \n",
    "    n_jobs=1, \n",
    "    train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "\n",
    "# train_sizes 是用来在曲线上绘制每个点的数据大小。\n",
    "# train_scores 是针对每组数据进行训练后的算法训练得分。\n",
    "# test_scores 是针对每组数据进行训练后的算法测试得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# estimator= LogisticRegression()\n",
    "# estimator = GradientBoostingClassifier()\n",
    "estimator = SVC(kernel='rbf', gamma=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_learning_curves(X, y, estimator, num_trainings):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.plot(train_scores_mean, 'o-', color=\"g\", label=\"Training score\")\n",
    "    plt.plot(test_scores_mean, 'o-', color=\"y\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a7456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7147ae32",
   "metadata": {},
   "source": [
    "# 评估指标-回归指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f487586",
   "metadata": {},
   "source": [
    "## 平均绝对误差Mean Absolute Error(MAE)\n",
    "\n",
    "绝对值函数不可微，不利于使用梯度下降等方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7465acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_predict = lr.predict(X)\n",
    "\n",
    "error = mean_absolute_error(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d62055",
   "metadata": {},
   "source": [
    "## 均方误差Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7006b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_predict = lr.predict(X)\n",
    "\n",
    "error = mean_squared_error(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bedce",
   "metadata": {},
   "source": [
    "## R2分数\n",
    "\n",
    "R2分数等于0，相当于用点的平均值来猜测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "score = r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15963ab7",
   "metadata": {},
   "source": [
    "# 模型选择\n",
    "在机器学习中，我们用训练数据集去训练(学习)一个model(模型)，通常的做法是定义一个Loss function（误差函数），通过将这个Loss(或者叫error)的最小化过程，来提高模型的性能(performance)。\n",
    "\n",
    "然而我们学习一个模型的目的是为了解决实际的问题（或者说是训练数据集这个领域(field)中的一般化问题)，单纯地将训练数据集的loss最小化，并不能保证在解决更一般的问题时模型仍然是最优，甚至不能保证模型是可用的。\n",
    "\n",
    "这个训练数据集的loss与一般化的数据集的loss之间的差异就叫做`generalization error=bias+variance。`\n",
    "\n",
    ">Bias和Variance是针对Generalization（一般化，泛化）来说的。\n",
    "\n",
    "用图表说明两种错误\n",
    "\n",
    "- **欠拟合Underfitting:** Oversimplify the problem.训练集拟合不够好，偏差引起的误差(**High bias)**。\n",
    "- **过拟合Overfitting:** Overcomplicate the problem.记住而非学习数据，训练集表现良好，但在测试集上表现不好，方差引起的误差(**High Variance**)。\n",
    "\n",
    "**如果过拟合，欠拟合，怎么办？**\n",
    "\n",
    "判断模型的错误类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab1962",
   "metadata": {},
   "source": [
    "## 交叉验证Cross Validation\n",
    "\n",
    "交叉验证是一种通过估计模型的泛化误差，从而进行模型选择的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf2f1a",
   "metadata": {},
   "source": [
    "## K折交叉验证K Fold Cross Validation\n",
    "将数据分作K份，每次轮流取不同的1份用作测试集。多结果取平均，得到最终的模型。如何取平均，要对测试数据进行训练？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1535f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(data, 3, shuffle=True)\n",
    "for train_indicies, test_indicies in kf:\n",
    "    print(train_indicies, test_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d523e",
   "metadata": {},
   "source": [
    "## 模型复杂度图表Model Complexity Graph\n",
    "不能将测试集用于模型的选择。\n",
    "\n",
    "**MCG 和学习曲线的区别**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b05223",
   "metadata": {},
   "source": [
    "## 网格搜索Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 选择参数\n",
    "# 选择想要选择的参数，并形成一个字典。 键 (keys) 将是参数的名称，值 （values) 是每个参数可能值的列表。\n",
    "parameters = {'kernel':['poly', 'rbf'],'C':[0.1, 1, 10]}\n",
    "\n",
    "# 创建一个评分机制 (scorer)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# 使用参数 (parameter) 和评分机制 (scorer) 创建一个 GridSearch 对象。 \n",
    "# Create the object.\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n",
    "# Fit the data\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# 获得最佳估算器 (estimator)\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Fit the new model\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the new model.\n",
    "best_train_predictions = best_clf.predict(X_train)\n",
    "best_test_predictions = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f56b86",
   "metadata": {},
   "source": [
    "# 防止过拟合\n",
    "过拟合\n",
    "\n",
    "在进行数据挖掘或者机器学习模型建立的时候，因为在统计学习中，假设数据满足独立同分布（i.i.d，independently and identically distributed），即当前已产生的数据可以对未来的数据进行推测与模拟，因此都是使用历史数据建立模型，即使用已经产生的数据去训练，然后使用该模型去拟合未来的数据。\n",
    "\n",
    "但是一般独立同分布的假设往往不成立，即数据的分布可能会发生变化（distribution drift），并且可能当前的数据量过少，不足以对整个数据集进行分布估计，因此往往需要防止模型过拟合，提高模型泛化能力。而为了达到该目的的最常见方法便是：\n",
    "\n",
    "- 正则化，即在对模型的目标函数（objective function）或代价函数（cost function）加上正则项。\n",
    "\n",
    "在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting）。如下图所示：\n",
    "\n",
    "通过上图可以看出，随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work。\n",
    "\n",
    "为了防止过拟合，我们需要用到一些方法，如：`early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout`等。\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。\n",
    "\n",
    "Early stopping便是一种`迭代次数截断`的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。\n",
    "\n",
    "Early stopping方法的具体做法是：\n",
    "\n",
    "在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。\n",
    "\n",
    "这种做法很符合直观感受，因为accurary都不再提高了，再继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“`No-improvement-in-n`”，n即Epoch的次数，可以根据实际情况取，如10、20、30……\n",
    "\n",
    "## 数据集扩增\n",
    "> 有时候往往拥有更多的数据胜过一个好的模型\n",
    "\n",
    "因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模拟地更准确。\n",
    "\n",
    "因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。\n",
    "\n",
    "所以，往往在这时候，需要采取一些计算的方式与策略在已有的数据集上进行手脚，以得到更多的数据。通俗得讲，数据机扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法：\n",
    "\n",
    "- 从数据源头采集更多数据；\n",
    "- 复制原有数据并加上随机噪声；\n",
    "- 重采样；\n",
    "- 根据当前数据集估计数据分布参数，使用该分布产生更多数据等；\n",
    "\n",
    "## 正则化方法\n",
    "正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。\n",
    "\n",
    "- L1正则\n",
    "\n",
    "L1正则是基于L1范数，即在目标函数后面加上参数的L1范数和项，即参数绝对值和与参数的积项，即：其中代表原始的代价函数，是样本的个数，就是正则项系数，权衡正则项与项的比重。后面那一项即为L1正则项。 \n",
    "\n",
    "在计算梯度时，的梯度变为：其中，是符号函数，那么便使用下式对参数进行更新：对于有些模型，如线性回归中（L1正则线性回归即为Lasso回归），常数项的更新方程不包括正则项，即：其中，梯度下降算法中，，而在梯度上升算法中则相反。 从上式可以看出，当为正时，更新后会变小；当为负时，更新后会变大；因此L1正则项是为了使得那些原先处于零（即）附近的参数往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。 其中，L1正则中有个问题，便是L1范数在0处不可导，即在0处不可导，因此在为0时，使用原来的未经正则化的更新方程来对进行更新，即令，这样即：\n",
    "\n",
    "- L2正则\n",
    "\n",
    "L2正则是基于L2范数，即在目标函数后面加上参数的L2范数和项，即参数的平方和与参数的积项，即：其中代表原始的代价函数，是样本的个数，与L1正则化项前面的参数不同的是，L2项的参数乘了，是为了便于计算以及公式的美感性，因为平方项求导有个2，就是正则项系数，权衡正则项与项的比重。后面那一项即为L2正则项。 L2正则化中则使用下式对模型参数进行更新：对于有些模型，如线性回归中（L2正则线性回归即为Ridge回归，岭回归），常数项的更新方程不包括正则项，即：其中，梯度下降算法中，，而在梯度上升算法中则相反。 从上式可以看出，L2正则项起到使得参数变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。 在这里需要提到的是，在对模型参数进行更新学习的时候，有两种更新方式，mini-batch （部分增量更新）与 full-batch（全增量更新），即在每一次更新学习的过程中（一次迭代，即一次epoch），在mini-batch中进行分批处理，先使用一部分样本进行更新，然后再使用一部分样本进行更新。直到所有样本都使用了，这次epoch的损失函数值则为所有mini batch的平均损失值。设每次mini batch中样本个数为，那么参数的更新方程中的正则项要改成：而full-batch即每一次epoch中，使用全部的训练样本进行更新，那么每次的损失函数值即为全部样本的误差之和。更新方程不变。\n",
    "\n",
    "**总结** \n",
    "\n",
    "正则项是为了降低模型的复杂度，从而避免模型去过分拟合训练数据，包括噪声与异常点（outliers）。\n",
    "\n",
    "从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。\n",
    "\n",
    "从贝叶斯学派来看：加了先验，在数据少的时候，先验知识可以防止过拟合；\n",
    "\n",
    "从频率学派来看：正则项限定了参数的取值，从而提高了模型的稳定性，而稳定性强的模型不会过拟合，即控制模型空间。 \n",
    "\n",
    "另外一个角度，过拟合从直观上理解便是，在对训练数据进行拟合时，需要照顾到每个点，从而使得拟合函数波动性非常大，即方差大。在某些小区间里，函数值的变化性很剧烈，意味着函数在某些小区间里的导数值的绝对值非常大，由于自变量的值在给定的训练数据集中的一定的，因此只有系数足够大，才能保证导数的绝对值足够大。如下图（引用知乎）：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/32fb4a59-29f2-4016-870a-056b14b5d9a8/Untitled.png\n",
    "\n",
    "另外一个解释，规则化项的引入，在训练（最小化cost）的过程中，当某一维的特征所对应的权重过大时，而此时模型的预测和真实数据之间距离很小，通过规则化项就可以使整体的cost取较大的值，从而，在训练的过程中避免了去选择那些某一维（或几维）特征的权重过大的情况，即过分依赖某一维（或几维）的特征（引用知乎）。\n",
    "\n",
    "L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。它们都是服从均值为0，协方差为。当时，即没有先验）没有正则项，则相当于先验分布具有无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练集数据， 参数可以变得任意大从而使得模型不稳定，即方差大而偏差小。越大，标明先验分布协方差越小，偏差越大，模型越稳定。即，加入正则项是在偏差bias与方差variance之间做平衡tradeoff（来自知乎）。下图即为L2与L1正则的区别：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a2713b2-9887-407a-b9b6-dc061652ecb9/Untitled.png\n",
    "\n",
    "上图中的模型是线性回归，有两个特征，要优化的参数分别是w1和w2，左图的正则化是L2，右图是L1。蓝色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是红色边界（就是正则化那部分），二者相交处，才是最优参数。可见右边的最优参数只可能在坐标轴上，所以就会出现0权重参数，使得模型稀疏。\n",
    "\n",
    "其实拉普拉斯分布与高斯分布是数学家从实验中误差服从什么分布研究中得来的。一般直观上的认识是服从应该服从均值为0的对称分布，并且误差大的频率低，误差小的频率高，因此拉普拉斯使用拉普拉斯分布对误差的分布进行拟合，如下图：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d1ecacd-07d1-40b0-99d1-b248bed782c1/Untitled.png\n",
    "\n",
    "而拉普拉斯在最高点，即自变量为0处不可导，因为不便于计算，于是高斯在这基础上使用高斯分布对其进行拟合，如下图：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/196db4eb-abc5-4c44-9a41-c06476bce1fa/Untitled.png\n",
    "\n",
    "## **Dropout**\n",
    "\n",
    "正则是通过在代价函数后面加上正则项来防止模型过拟合的。而在神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trick），对于如下所示的三层人工神经网络：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/263370c4-8cbd-41f5-8674-f53a12f48b05/Untitled.png\n",
    "\n",
    "对于上图所示的网络，在训练开始时，随机得删除一些（可以设定为一半，也可以为1/3，1/4等）隐藏层神经元，即认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变，这样便得到如下的ANN：\n",
    "\n",
    "!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/224db7a0-cebc-420e-81b5-e20648eb504c/Untitled.png\n",
    "\n",
    "然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择。这样一直进行瑕疵，直至训练结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb7575",
   "metadata": {},
   "source": [
    "# 分类问题指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_classification(algorithm, X_train, X_test, y_train, y_test, cols, cf, threshold_plot):\n",
    "    \n",
    "    # model\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    predictions = algorithm.predict(X_test)\n",
    "    probabilities = algorithm.predict_proba(X_test)\n",
    "    \n",
    "    # coeffs\n",
    "    if cf == \"coefficients\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n",
    "    elif cf == \"features\" :\n",
    "        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n",
    "        \n",
    "    column_df = pd.DataFrame(cols)\n",
    "    coef_sumry =(pd.merge(coefficients, column_df,left_index= True, right_index= True, how = \"left\"))\n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry = coef_sumry.sort_values(by = \"coefficients\", ascending = False)\n",
    "    \n",
    "    # result\n",
    "    print(algorithm)\n",
    "    print(\"\\n Classification report : \\n\", classification_report(y_test, predictions))\n",
    "    print(\"Accuracy Score : \", accuracy_score(y_test, predictions))\n",
    "    \n",
    "    #confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    #roc_auc_score\n",
    "    model_roc_auc = roc_auc_score(y_test, predictions) \n",
    "    print(\"Area under curve : \", model_roc_auc, \"\\n\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities[:,1])\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    trace1 = go.Heatmap(z = conf_matrix ,\n",
    "                        x = [\"Not Labled\", \"Labled\"],\n",
    "                        y = [\"Not Labled\", \"Labled\"],\n",
    "                        showscale  = False, colorscale = \"Picnic\", name = \"matrix\")\n",
    "    \n",
    "    #plot roc curve\n",
    "    trace2 = go.Scatter(x = fpr, y = tpr,\n",
    "                        name = \"Roc : \" + str(model_roc_auc),\n",
    "                        line = dict(color =('rgb(22, 96, 167)'),width = 2))\n",
    "    trace3 = go.Scatter(x = [0,1],y=[0,1],\n",
    "                        line = dict(color =('rgb(205, 12, 24)'),width = 2,\n",
    "                        dash = 'dot'))\n",
    "    \n",
    "    #plot coeffs\n",
    "    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n",
    "                    name = \"coefficients\",\n",
    "                    marker = dict(color = coef_sumry[\"coefficients\"],\n",
    "                                  colorscale = \"Picnic\",\n",
    "                                  line = dict(width = .6,color = \"black\")))\n",
    "    \n",
    "    #subplots\n",
    "    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver operating characteristic',\n",
    "                                            'Feature Importances'))\n",
    "    \n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,2)\n",
    "    fig.append_trace(trace3,1,2)\n",
    "    fig.append_trace(trace4,2,1)\n",
    "    \n",
    "    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n",
    "                         autosize = False,height = 900,width = 800,\n",
    "                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                         margin = dict(b = 195))\n",
    "    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n",
    "    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n",
    "    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n",
    "                                        tickangle = 90))\n",
    "    py.iplot(fig)\n",
    "    \n",
    "    if threshold_plot == True : \n",
    "        visualizer = DiscriminationThreshold(algorithm)\n",
    "        visualizer.fit(X_train,y_train)\n",
    "        visualizer.poof()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
